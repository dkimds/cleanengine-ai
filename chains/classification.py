from config import DEFAULT_MODEL, CRYPTO_KEYWORDS
"""
Classification chain for determining the type of user query.
"""

import asyncio
import concurrent.futures
import torch
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from typing import Dict, Any, List
from .vllm_singleton import vllm_singleton


class ClassificationChain:
    """
    Classifies user questions into categories: ìµœì‹ ì†Œì‹, ì „ë¬¸ì§€ì‹, ë¦¬ì…‹, or ê¸°íƒ€
    """
    
    def __init__(self, model: str = DEFAULT_MODEL):
        """
        Initialize the classification chain.
        
        Args:
            model: vLLM model to use for classification
        """
        self.model = model
        self.llm = vllm_singleton.get_llm(model)
        self.sampling_params = vllm_singleton.create_sampling_params(temperature=0.0, max_tokens=20)
        self._setup_chain()
        
        # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        self.is_gpu_available = torch.cuda.is_available() and torch.cuda.device_count() > 0
        self.batch_enabled = self.is_gpu_available
        
        print(f"ğŸ” Classification Chain GPU Status: {'GPU' if self.is_gpu_available else 'CPU'}")
        print(f"ğŸš€ Batch Processing: {'Enabled' if self.batch_enabled else 'Disabled'}")
        
        # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor
        self.executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=4, 
            thread_name_prefix="classification_async"
        )
    
    def _setup_chain(self):
        """Set up the classification chain with prompt and model."""
        
        # Classification prompt template - focused on crypto distinction
        self.prompt = PromptTemplate.from_template(
            """Classify this cryptocurrency question into exactly ONE category:

Categories:
1. ìµœì‹ ì†Œì‹ - Current prices, recent news, market updates, real-time data
   Examples: "ë¹„íŠ¸ì½”ì¸ í˜„ì¬ ê°€ê²©", "ì´ë”ë¦¬ì›€ ê¸‰ë½ ì´ìœ ", "ì˜¤ëŠ˜ ì•”í˜¸í™”í ë‰´ìŠ¤", "ìµœê·¼ ETF ìŠ¹ì¸"

2. ì „ë¬¸ì§€ì‹ - Investment strategies, technical analysis, educational content, how-to guides
   Examples: "íˆ¬ì ì „ëµ", "ì°¨íŠ¸ ë¶„ì„ ë°©ë²•", "ìŠ¤í…Œì´í‚¹ ìˆ˜ìµë¥  ê³„ì‚°", "DeFi ì›ë¦¬ ì„¤ëª…"

Question: {question}

Answer with only the category name (ìµœì‹ ì†Œì‹ or ì „ë¬¸ì§€ì‹):"""
        )
    
    def _classify_sync(self, question: str, chat_history: str = "") -> str:
        """
        ë™ê¸°ì ìœ¼ë¡œ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ.
        
        Args:
            question: The user's question
            chat_history: Previous conversation history
            
        Returns:
            Classification result as string
        """
        question_lower = question.lower()
        
        # 1. RULES FIRST - Fast path for obvious cases
        
        # Reset requests (highest priority)
        if any(word in question_lower for word in ["ë¦¬ì…‹", "ì´ˆê¸°í™”", "ì§€ì›Œ", "reset", "clear"]):
            return "ë¦¬ì…‹"
        
        # Non-crypto questions â†’ ê¸°íƒ€
        has_crypto = any(keyword in question_lower for keyword in CRYPTO_KEYWORDS)
        
        if not has_crypto:
            return "ê¸°íƒ€"

        # 2. LLM for ambiguous crypto questions
        try:
            prompt_text = self.prompt.format(
                question=question,
                chat_history=chat_history
            )
            outputs = self.llm.generate([prompt_text], self.sampling_params)
            result = outputs[0].outputs[0].text.strip()
            
            # Extract category from LLM output
            if "ì „ë¬¸ì§€ì‹" in result:
                return "ì „ë¬¸ì§€ì‹"
            elif "ìµœì‹ ì†Œì‹" in result:
                return "ìµœì‹ ì†Œì‹"
            else:
                # Default for crypto questions
                return "ìµœì‹ ì†Œì‹"
                
        except Exception as e:
            print(f"[Classification Error] {e}")
            return "ìµœì‹ ì†Œì‹"  # Safe fallback for crypto questions
    
    def classify(self, question: str, chat_history: str = "") -> str:
        """
        Classify a user question using true hybrid approach.
        Rules for obvious cases, LLM for crypto ambiguity.
        
        Args:
            question: The user's question
            chat_history: Previous conversation history
            
        Returns:
            Classification result as string
        """
        return self._classify_sync(question, chat_history)
    
    async def classify_async(self, question: str, chat_history: str = "") -> str:
        """
        ë¹„ë™ê¸°ì ìœ¼ë¡œ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            question: The user's question
            chat_history: Previous conversation history
            
        Returns:
            Classification result as string
        """
        # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ê¸° ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor, 
            self._classify_sync, 
            question, 
            chat_history
        )
        return result
    
    def invoke(self, inputs: Dict[str, Any]) -> str:
        """
        Invoke the classification chain with inputs.
        
        Args:
            inputs: Dictionary containing 'question' and 'chat_history'
            
        Returns:
            Classification result as string
        """
        return self.classify(
            inputs.get("question", ""),
            inputs.get("chat_history", "")
        )
    
    async def ainvoke(self, inputs: Dict[str, Any]) -> str:
        """
        ì§„ì§œ ë¹„ë™ê¸°ë¡œ classification chainì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            inputs: Dictionary containing 'question' and 'chat_history'
            
        Returns:
            Classification result as string
        """
        return await self.classify_async(
            inputs.get("question", ""),
            inputs.get("chat_history", "")
        )
    
    async def classify_batch_async(self, questions: List[str], chat_histories: List[str] = None) -> List[str]:
        """
        ì—¬ëŸ¬ ì§ˆë¬¸ì„ ë°°ì¹˜ë¡œ ë¹„ë™ê¸° ì²˜ë¦¬í•©ë‹ˆë‹¤. (GPUì—ì„œë§Œ ì§„ì§œ ë°°ì¹˜ ì²˜ë¦¬)
        
        Args:
            questions: List of questions to classify
            chat_histories: List of chat histories (optional)
            
        Returns:
            List of classification results
        """
        if chat_histories is None:
            chat_histories = [""] * len(questions)
        
        # GPUê°€ ì—†ê±°ë‚˜ ì§ˆë¬¸ì´ 1ê°œë©´ ê°œë³„ ì²˜ë¦¬
        if not self.batch_enabled or len(questions) <= 1:
            print(f"ğŸ“ Individual processing: GPU={self.is_gpu_available}, Questions={len(questions)}")
            tasks = [
                self.classify_async(question, chat_history)
                for question, chat_history in zip(questions, chat_histories)
            ]
            return await asyncio.gather(*tasks)
        
        # GPU í™˜ê²½ì—ì„œ ì§„ì§œ ë°°ì¹˜ ì²˜ë¦¬
        print(f"ğŸš€ GPU Batch processing: {len(questions)} questions")
        
        # ë¹ ë¥¸ ê·œì¹™ ê¸°ë°˜ ì²˜ë¦¬
        fast_results = []
        llm_questions = []
        llm_histories = []
        llm_indices = []
        
        for i, (question, chat_history) in enumerate(zip(questions, chat_histories)):
            question_lower = question.lower()
            
            # ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì¦‰ì‹œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ê²½ìš°
            if any(word in question_lower for word in ["ë¦¬ì…‹", "ì´ˆê¸°í™”", "ì§€ì›Œ", "reset", "clear"]):
                fast_results.append((i, "ë¦¬ì…‹"))
            elif not any(keyword in question_lower for keyword in CRYPTO_KEYWORDS):
                fast_results.append((i, "ê¸°íƒ€"))
            else:
                # LLM ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°
                llm_questions.append(question)
                llm_histories.append(chat_history)
                llm_indices.append(i)
        
        # LLM ë°°ì¹˜ ì²˜ë¦¬ (GPUì—ì„œë§Œ)
        llm_results = []
        if llm_questions:
            prompts = [
                self.prompt.format(question=q, chat_history=h) 
                for q, h in zip(llm_questions, llm_histories)
            ]
            
            # ThreadPoolExecutorë¡œ GPU ë°°ì¹˜ ì²˜ë¦¬
            loop = asyncio.get_event_loop()
            batch_outputs = await loop.run_in_executor(
                self.executor,
                self._batch_generate_sync,
                prompts
            )
            
            # ê²°ê³¼ íŒŒì‹±
            for output_text in batch_outputs:
                if "ì „ë¬¸ì§€ì‹" in output_text:
                    llm_results.append("ì „ë¬¸ì§€ì‹")
                elif "ìµœì‹ ì†Œì‹" in output_text:
                    llm_results.append("ìµœì‹ ì†Œì‹")
                else:
                    llm_results.append("ìµœì‹ ì†Œì‹")  # ê¸°ë³¸ê°’
        
        # ìµœì¢… ê²°ê³¼ ì¡°í•©
        final_results = [""] * len(questions)
        
        # ë¹ ë¥¸ ì²˜ë¦¬ ê²°ê³¼ ì ìš©
        for idx, result in fast_results:
            final_results[idx] = result
        
        # LLM ì²˜ë¦¬ ê²°ê³¼ ì ìš©
        for idx, result in zip(llm_indices, llm_results):
            final_results[idx] = result
        
        print(f"âœ… Batch completed: {len(fast_results)} rules, {len(llm_results)} GPU batch")
        return final_results
    
    def _batch_generate_sync(self, prompts: List[str]) -> List[str]:
        """GPUì—ì„œ ë™ê¸° ë°°ì¹˜ ìƒì„± (ThreadPoolExecutorì—ì„œ ì‹¤í–‰)"""
        try:
            # GPU í™˜ê²½ì—ì„œ ì§„ì§œ ë°°ì¹˜ ì²˜ë¦¬
            outputs = self.llm.generate(prompts, self.sampling_params)
            
            results = []
            for output in outputs:
                if output.outputs:
                    results.append(output.outputs[0].text.strip())
                else:
                    results.append("")
            
            print(f"ğŸ¯ GPU Batch processed {len(prompts)} prompts successfully")
            return results
            
        except Exception as e:
            print(f"âŒ GPU Batch classification error: {e}")
            return ["ìµœì‹ ì†Œì‹"] * len(prompts)  # ì•ˆì „í•œ ê¸°ë³¸ê°’
    
    def get_batch_info(self) -> Dict[str, Any]:
        """ë°°ì¹˜ ì²˜ë¦¬ ì •ë³´ ë°˜í™˜"""
        return {
            "gpu_available": self.is_gpu_available,
            "batch_enabled": self.batch_enabled,
            "device": "GPU" if self.is_gpu_available else "CPU",
            "batch_mode": "True GPU Batch" if self.batch_enabled else "Individual Processing"
        }
    
    def __del__(self):
        """í´ë¦°ì—…: ThreadPoolExecutor ì¢…ë£Œ"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)